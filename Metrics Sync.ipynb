{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2bcf16bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import scipy.stats as stats\n",
    "import statsmodels.formula.api as smf\n",
    "from scipy.stats import bernoulli\n",
    "from scipy.stats import truncnorm\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "873f0dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DGP(intervene_A=None):\n",
    "    p = 0.6     # Above, this was defined as 0.5. In paper, it's 0.6\n",
    "    n = 10000\n",
    "\n",
    "    # A (gender)\n",
    "    if intervene_A:\n",
    "        A2 = np.ones(n)\n",
    "    else:\n",
    "        A2 = U_A = bernoulli.rvs(p=p, size=n)\n",
    "\n",
    "\n",
    "    # Q (qualifications)\n",
    "    U_Q = np.random.normal(2, 5, n)\n",
    "    Q2 = np.floor(U_Q)\n",
    "\n",
    "\n",
    "    # D (number of children)\n",
    "    lower_d, upper_d = 0.1, 3\n",
    "    mu_d, sigma_d = 2, 1\n",
    "    X_d = stats.truncnorm((lower_d - mu_d) / sigma_d, (upper_d - mu_d) / sigma_d, loc=mu_d, scale=sigma_d)\n",
    "    U_D = X_d.rvs(n)\n",
    "    D2 = A2 + np.floor(0.5 * Q2 * U_D)\n",
    "\n",
    "\n",
    "    # M (physical strength)\n",
    "    lower_m, upper_m = 0.1, 3\n",
    "    mu_m, sigma_m = 3, 2\n",
    "    X_m = stats.truncnorm((lower_m - mu_m) / sigma_m, (upper_m - mu_m) / sigma_m, loc=mu_m, scale=sigma_m)\n",
    "    U_M = X_m.rvs(n)\n",
    "    M2 = 3*A2 + (0.4 * Q2 * U_M)\n",
    "\n",
    "\n",
    "    sigmoid = lambda x: 1/(1+ math.exp(-x))\n",
    "\n",
    "    Y2 = np.zeros(n)\n",
    "    for i in range(n):\n",
    "        Y2[i] = sigmoid(-10+5*A2[i]+Q2[i]+D2[i]+M2[i]) >= 0.5\n",
    "\n",
    "\n",
    "    df = pd.DataFrame({'A': A2, 'D': D2, 'M': M2 ,'Q': Q2,'Y':Y2})\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7031735e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#counterfactual gender\n",
    "def counterfactual_df(without_intervention):\n",
    "    n = 10000\n",
    "    A2 = 1 - without_intervention.A.values\n",
    "\n",
    "    # Q (qualifications) - unchanged\n",
    "    Q2 = without_intervention.Q.values\n",
    "\n",
    "\n",
    "    # D (number of children)\n",
    "    lower_d, upper_d = 0.1, 3\n",
    "    mu_d, sigma_d = 2, 1\n",
    "    X_d = stats.truncnorm((lower_d - mu_d) / sigma_d, (upper_d - mu_d) / sigma_d, loc=mu_d, scale=sigma_d)\n",
    "    U_D = X_d.rvs(n)\n",
    "    D2 = A2 + np.floor(0.5 * Q2 * U_D)\n",
    "\n",
    "\n",
    "    # M (physical strength)\n",
    "    lower_m, upper_m = 0.1, 3\n",
    "    mu_m, sigma_m = 3, 2\n",
    "    X_m = stats.truncnorm((lower_m - mu_m) / sigma_m, (upper_m - mu_m) / sigma_m, loc=mu_m, scale=sigma_m)\n",
    "    U_M = X_m.rvs(n)\n",
    "    M2 = 3*A2 + (0.4 * Q2 * U_M)\n",
    "\n",
    "\n",
    "    sigmoid = lambda x: 1/(1+ math.exp(-x))\n",
    "\n",
    "    Y2 = np.zeros(n)\n",
    "    for i in range(n):\n",
    "        Y2[i] = sigmoid(-10+5*A2[i]+Q2[i]+D2[i]+M2[i]) >= 0.5\n",
    "\n",
    "    Counterfactual_df = pd.DataFrame({'A': A2, 'D': D2, 'M': M2 ,'Q': Q2,'Y':Y2})\n",
    "    return Counterfactual_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cfc5dbf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Preperation\n",
    "def data_processing(without_intervention, Counterfactual_df):\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    #discretize fair fair Q\n",
    "    without_intervention['bi_Q'] = np.where(without_intervention['Q'] <= np.quantile(without_intervention['Q'], 0.5), -1, 1)\n",
    "    Counterfactual_df['bi_Q'] = np.where(Counterfactual_df['Q'] <= np.quantile(Counterfactual_df['Q'], 0.5), -1, 1)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(without_intervention.drop(['Y'], axis = 1), \n",
    "                                                        without_intervention.Y,\n",
    "                                                        test_size = 0.2, \n",
    "                                                        stratify = without_intervention.Y)\n",
    "    #get the same rows of counterfactual test data\n",
    "    Counter_X_test = Counterfactual_df.drop(['Y'], axis = 1).iloc[X_test.index.values, :]\n",
    "    Counter_y_test = Counterfactual_df['Y'][X_test.index.values]\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test, Counter_X_test, Counter_y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3582a9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model training and prediction results:\n",
    "def trainers(X_train, y_train, full_cols, unaware_cols, fair_var, model):\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    from sklearn import svm\n",
    "    \n",
    "    if model == 'log':\n",
    "        full = LogisticRegression()\n",
    "        unaware = LogisticRegression()\n",
    "        fair = LogisticRegression()\n",
    "        \n",
    "        full.fit(X_train[full_cols], y_train)\n",
    "        unaware.fit(X_train[unaware_cols], y_train)\n",
    "        fair.fit(X_train[fair_var], y_train)\n",
    "    \n",
    "    elif model == 'svm':\n",
    "        full = svm()\n",
    "        unaware = svm()\n",
    "        fair = svm()\n",
    "        \n",
    "        full.fit(X_train[full_cols], y_train)\n",
    "        unaware.fit(X_train[unaware_cols], y_train)\n",
    "        fair.fit(X_train[fair_var], y_train)\n",
    "        \n",
    "        \n",
    "    elif model == 'dt':\n",
    "        full = DecisionTreeClassifier()\n",
    "        unaware = DecisionTreeClassifier()\n",
    "        fair = DecisionTreeClassifier()\n",
    "        \n",
    "        full.fit(X_train[full_cols], y_train)\n",
    "        unaware.fit(X_train[unaware_cols], y_train)\n",
    "        fair.fit(X_train[fair_var], y_train)\n",
    "    \n",
    "    else:\n",
    "        dec = 'model is not included in is trainer'\n",
    "        return dec\n",
    "    \n",
    "    return {'full': full, 'unaware': unaware, 'fair': fair}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4ed88865",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get predictions\n",
    "def get_predictions(X_test, Counter_X_test, full_cols, unaware_cols, fair_var, fitted_models):\n",
    "    import pandas as pd\n",
    "    \n",
    "    predictions = {}\n",
    "    counter_predictions = {}\n",
    "    \n",
    "    for model in fitted_models:\n",
    "        if model == 'full':\n",
    "            predictions[model] = fitted_models[model].predict(X_test[full_cols])\n",
    "            counter_predictions[model] = fitted_models[model].predict(Counter_X_test[full_cols])\n",
    "        if model == 'unaware':\n",
    "            predictions[model] = fitted_models[model].predict(X_test[unaware_cols])\n",
    "            counter_predictions[model] = fitted_models[model].predict(Counter_X_test[unaware_cols])\n",
    "        if model == 'fair':\n",
    "            predictions[model] = fitted_models[model].predict(X_test[fair_var])\n",
    "            counter_predictions[model] = fitted_models[model].predict(Counter_X_test[fair_var])\n",
    "\n",
    "    return pd.DataFrame(predictions), pd.DataFrame(counter_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dd95e3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## counterfactual fairness: sum of absulute difference (counter yhat - factual yhat)\n",
    "def counterfactual_fairness(counter_prediction, factual_prediction):\n",
    "    return sum(abs(counter_prediction - factual_prediction))/len(counter_prediction)\n",
    "\n",
    "## ETT fairness: absulute value difference between sum of counter predicted y and fautual predicted y\n",
    "def ett_fairness(counter_prediction, factual_prediction):\n",
    "    return abs(sum(factual_prediction)- sum(counter_prediction))/len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7789382",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e2a95f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "without_intervention = DGP(intervene_A=None)\n",
    "Counterfactual_df = counterfactual_df(without_intervention)\n",
    "X_train, X_test, y_train, y_test, Counter_X_test, Counter_y_test = data_processing(without_intervention, Counterfactual_df)\n",
    "\n",
    "full_cols = ['A', 'Q', 'D', 'M']\n",
    "unaware_cols = ['Q', 'D', 'M']\n",
    "fair_var = ['Q']\n",
    "model = 'log'\n",
    "\n",
    "fitted_models = trainers(X_train, y_train, full_cols, unaware_cols, fair_var, model)\n",
    "pred, counter_pred = get_predictions(X_test, Counter_X_test, full_cols, unaware_cols, fair_var, fitted_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f1872f70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ETT</th>\n",
       "      <th>Counterfactual</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Full Model</th>\n",
       "      <td>0.039</td>\n",
       "      <td>0.272</td>\n",
       "      <td>0.9955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Unaware Model</th>\n",
       "      <td>0.036</td>\n",
       "      <td>0.234</td>\n",
       "      <td>0.9730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fair Predictor Q</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.8670</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    ETT  Counterfactual  Accuracy\n",
       "Full Model        0.039           0.272    0.9955\n",
       "Unaware Model     0.036           0.234    0.9730\n",
       "Fair Predictor Q  0.000           0.000    0.8670"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame({\n",
    "    'ETT': [\n",
    "        ett_fairness(pred['full'], counter_pred['full']),\n",
    "        ett_fairness(pred['unaware'], counter_pred['unaware']),\n",
    "        ett_fairness(pred['fair'], counter_pred['fair'])\n",
    "    ],\n",
    "    'Counterfactual':[\n",
    "        counterfactual_fairness(pred['full'], counter_pred['full']),\n",
    "        counterfactual_fairness(pred['unaware'], counter_pred['unaware']),\n",
    "        counterfactual_fairness(pred['fair'], counter_pred['fair'])\n",
    "    ],\n",
    "    'Accuracy':[\n",
    "        fitted_models['full'].score(X_test[full_cols], y_test),\n",
    "        fitted_models['unaware'].score(X_test[unaware_cols], y_test),\n",
    "        fitted_models['fair'].score(X_test[fair_var], y_test)\n",
    "        \n",
    "    ]\n",
    "}, index = ['Full Model', 'Unaware Model', 'Fair Predictor Q'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9889e4bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
